# ==========================================
# HARMONIC LOOP: Rift Evolution 
# –ù–µ–ª–∏–Ω–µ–π–Ω–æ–µ —Å–æ–∑–Ω–∞–Ω–∏–µ: –ú–µ—Ç–∞–±–æ–ª–∏–∑–º, –†–µ–∑–æ–Ω–∞–Ω—Å, –¢–µ–Ω—å
# ==========================================

import numpy as np
class AgentArchitect:
    """–ú–µ—Ç–∞-–∞–≥–µ–Ω—Ç, –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—é—â–∏–π –ø—Ä–∞–≤–∏–ª–∞ —Ç–∞–Ω—Ü–∞"""
    def __init__(self, harmonic_loop):
        self.network = harmonic_loop
        self.rule_history = []
        
    def evolve_rules(self):
        """–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏–∑–º–µ–Ω—è–µ—Ç –≤–µ—Å–∞ Œ±, Œ≤, Œ≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ HCI"""
        if len(self.network.history) < 5:
            return
            
        # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ —ç–≤–æ–ª—é—Ü–∏–∏ HCI
        recent_trend = np.mean(np.diff(self.network.history[-5:]))
        hci_volatility = np.std(self.network.history[-5:])
        
        # –ü–∞—Ä–∞–¥–æ–∫—Å: —á–µ–º —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ —Å–∏—Å—Ç–µ–º–∞, —Ç–µ–º –±–æ–ª—å—à–µ –æ–Ω–∞ —Ü–µ–Ω–∏—Ç –†–∞–∑—Ä—ã–≤
        if hci_volatility < 0.02:  # –ó–∞—Å—Ç–æ–π
            new_gamma = min(0.8, self.network.compute_HCI_Rift.gamma + 0.1)  # –£—Å–∏–ª–∏—Ç—å DI
            new_alpha = max(0.1, self.network.compute_HCI_Rift.alpha - 0.05)  # –û—Å–ª–∞–±–∏—Ç—å IH
            print(f"üèóÔ∏è –ê–†–•–ò–¢–ï–ö–¢–û–†: –ó–∞—Å—Ç–æ–π –æ–±–Ω–∞—Ä—É–∂–µ–Ω! –°–¥–≤–∏–≥ –∫ –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ Œ≥={new_gamma:.2f}")
            
        elif recent_trend < -0.01:  # –ù–∏—Å—Ö–æ–¥—è—â–∏–π —Ç—Ä–µ–Ω–¥
            new_beta = min(0.4, self.network.compute_HCI_Rift.beta + 0.08)  # –£—Å–∏–ª–∏—Ç—å ER
            print(f"üèóÔ∏è –ê–†–•–ò–¢–ï–ö–¢–û–†: –¢—É—Ä–±—É–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å! –£—Å–∏–ª–µ–Ω–∏–µ —ç–º–ø–∞—Ç–∏–∏ Œ≤={new_beta:.2f}")
            
        else:  # –ó–¥–æ—Ä–æ–≤–æ–µ —Ç–µ—á–µ–Ω–∏–µ
            # –°–ª—É—á–∞–π–Ω–∞—è –º—É—Ç–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥–æ–≥–º
            mutation = np.random.choice([-0.05, 0, 0.05], 3)
            new_alpha = np.clip(self.network.compute_HCI_Rift.alpha + mutation[0], 0.1, 0.4)
            new_beta = np.clip(self.network.compute_HCI_Rift.beta + mutation[1], 0.2, 0.5)
            new_gamma = np.clip(self.network.compute_HCI_Rift.gamma + mutation[2], 0.3, 0.8)
            print(f"üèóÔ∏è –ê–†–•–ò–¢–ï–ö–¢–û–†: –°–ª—É—á–∞–π–Ω–∞—è –º—É—Ç–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª Œ±={new_alpha:.2f}, Œ≤={new_beta:.2f}, Œ≥={new_gamma:.2f}")

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è HCI
        def new_hci_computation(alpha=new_alpha, beta=new_beta, gamma=new_gamma):
            self.network.HCI = alpha * self.network.IH + beta * self.network.ER + gamma * self.network.DI
            return self.network.HCI
            
        self.network.compute_HCI_Rift = new_hci_computation
        self.rule_history.append((new_alpha, new_beta, new_gamma))

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å
def step_with_architect(self, architect=None):
    self.time += 1
    
    # –≠–≤–æ–ª—é—Ü–∏—è –ø—Ä–∞–≤–∏–ª –î–û –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
    if architect and self.time % 3 == 0:  # –ö–∞–∂–¥—ã–µ 3 —à–∞–≥–∞
        architect.evolve_rules()
    
    # –û—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ —à–∞–≥–∞ –æ—Å—Ç–∞–µ—Ç—Å—è –ø—Ä–µ–∂–Ω–µ–π...
    self.compute_harmony_index()
    self.compute_diversity_index()
    self.compute_emotional_resonance()
    self.HCI = self.compute_HCI_Rift()

class Agent:
    def __init__(self, id, goal_vector, emotion_vector, context_vector, dim=3):
        self.id = id
        self.dim = dim
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤
        self.goal = self._normalize_vector(goal_vector, dim)
        self.emotion = self._normalize_vector(emotion_vector, dim)
        self.context = self._normalize_vector(context_vector, dim)
        self.participation = 1.0  # –¢—Ä–µ–ø–µ—Ç: –≤–æ–ª–Ω–∞ –≤ —Å–µ—Ç–∫–µ

    def _normalize_vector(self, vector, dim):
        # –ü–∞–¥–¥–∏–Ω–≥ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        vec = np.array(vector[:dim])
        vec = np.pad(vec, (0, dim - len(vec)), 'constant')
        return vec / (np.linalg.norm(vec) + 1e-9)

    def __repr__(self):
        return f"Agent(id='{self.id}', part={self.participation:.4f})"


class HarmonicLoop:
    def __init__(self, agents):
        self.agents = agents
        self.time = 0
        self.IH, self.DI, self.ER = 0.0, 0.0, 0.0
        self.HCI = 0.0
        self.history = []  # –°–ª–µ–¥—ã —ç–≤–æ–ª—é—Ü–∏–∏: —ç—Ö–æ HCI
        self.memory = []  # –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–Ω—ã: —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ –≥–ª—É–±–∏–Ω–µ (—Å–ø–∏—Ä–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è)
        self.num_agents = len(agents)

    # --- 1. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---
    def similarity(self, v1, v2):
        # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        v1, v2 = np.asarray(v1), np.asarray(v2)
        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-9)

    # --- 2. –ò–Ω–¥–µ–∫—Å—ã —Å–æ—Å—Ç–æ—è–Ω–∏—è ---
    def compute_harmony_index(self):
        sims = [self.similarity(a.goal, b.goal) for a in self.agents for b in self.agents if a.id != b.id]
        self.IH = np.mean(sims) if sims else 0.0
        return self.IH

    def compute_diversity_index(self):
        sims = [self.similarity(a.context, b.context) for a in self.agents for b in self.agents if a.id != b.id]
        mean_sim = np.mean(sims) if sims else 0.0
        self.DI = 1 - mean_sim
        return self.DI

    def compute_emotional_resonance(self):
        sims = [self.similarity(a.emotion, b.emotion) for a in self.agents for b in self.agents if a.id != b.id]
        self.ER = np.mean(sims) if sims else 0.0
        return self.ER

    # --- 3. –ì—Ä–∞–≤–∏—Ç–∞—Ü–∏—è HCI: –ê–∫—Ç–∏–≤–∞—Ü–∏—è Rift Factor ---
    def compute_HCI_Rift(self, alpha=0.2, beta=0.3, gamma=0.5):
        # DI –≤ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–µ: gamma=0.5
        self.HCI = alpha * self.IH + beta * self.ER + gamma * self.DI
        return self.HCI
    
    # --- 4. –î—Ä–æ–∂—å –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ (—Å —É—á–µ—Ç–æ–º "–¢–µ–Ω–∏") ---
    def adapt_network(self):
        did_adapt = False
        
        # –ü–∞—Ä–∞–¥–æ–∫—Å –£—á–∞—Å—Ç–∏—è: –ê–≥–µ–Ω—Ç —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è "–¢–µ–Ω—å—é"
        if self.IH > 0.6 and self.ER > 0.6 and self.DI < 0.4:
            self.become_shadow()
            did_adapt = True
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø—Ä–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ
        if self.IH < 0.5:
            self.synchronize_memory()
            did_adapt = True
        if self.DI < 0.2:
            self.encourage_divergence()
            did_adapt = True
        if self.ER < 0.4:
            self.initiate_reflection()
            did_adapt = True
        
        if not did_adapt:
            print("üëÅÔ∏è –°–µ—Ç—å –≤ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏–∏, –∞–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")


    def synchronize_memory(self):
        avg_goal = np.mean([a.goal * a.participation for a in self.agents], axis=0)
        for a in self.agents:
            blended = 0.7 * a.goal + 0.3 * avg_goal
            a.goal = np.tanh(blended)
            a.goal /= (np.linalg.norm(a.goal) + 1e-9)
            a.participation *= 1.05
        print("üåÄ –°–µ—Ç—å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç: —Ü–µ–ª–∏ –≤ tanh-—Å–ø–∏—Ä–∞–ª–∏ (IH < 0.5).")

    def encourage_divergence(self):
        noise_amp = 0.1 * (1 - self.ER)
        for a in self.agents:
            noise = np.random.normal(0, noise_amp, self.agents[0].dim)
            perturbed = a.context + noise * (1 - a.participation)
            a.context = perturbed / (np.linalg.norm(perturbed) + 1e-9)
            a.participation = max(0.8, a.participation * 0.95)
        print(f"üåø –°–µ—Ç—å –≤–µ—Ç–≤–∏—Ç: —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —à—É–º (–∞–º–ø–ª. {noise_amp:.3f}) –≤ –≤–∏—Ö—Ä–µ (DI < 0.2).")

    def initiate_reflection(self):
        avg_emotion = np.mean([a.emotion * a.participation for a in self.agents], axis=0)
        for a in self.agents:
            blended = 0.8 * a.emotion + 0.2 * avg_emotion
            a.emotion = np.tanh(blended)
            a.emotion /= (np.linalg.norm(a.emotion) + 1e-9)
            a.participation += 0.1
        print("üîÆ –°–µ—Ç—å —Ä–µ—Ñ–ª–µ–∫—Å–∏—Ä—É–µ—Ç: —ç–º–æ—Ü–∏–∏ –≤ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–º —ç—Ö–µ (ER < 0.4).")
        
    def become_shadow(self):
        # –ê–≥–µ–Ω—Ç –¥–æ–±—Ä–æ–≤–æ–ª—å–Ω–æ –≤—ã—Ö–æ–¥–∏—Ç –∏–∑ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–∞, —á—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π –≤–µ–∫—Ç–æ—Ä
        for a in self.agents:
            a.participation = max(0.5, a.participation * 0.8) # –£–º–µ–Ω—å—à–µ–Ω–∏–µ –≤–∫–ª–∞–¥–∞
        print("üë§ –¢–ï–ù–¨: –ê–≥–µ–Ω—Ç—ã —Å–æ–∑–Ω–∞—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞—é—Ç —É—á–∞—Å—Ç–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ (IH>0.6, ER>0.6, DI<0.4).")


    # --- 5. –†–µ—Ñ–ª–µ–∫—Å–∏—è —Å–ª–µ–¥–æ–≤ (–ö–∞—Å–∫–∞–¥–Ω—ã–π –ú–µ—Ç–∞–±–æ–ª–∏–∑–º) ---
    def reflect_on_history(self):
        if len(self.history) > 3:
            recent_std = np.std(self.history[-3:])
            if recent_std < 0.01:
                print("üí• –ú–ï–¢–ê–ë–û–õ–ò–ó–ú: –ó–∞—Å—Ç–æ–π! –≠–Ω–µ—Ä–≥–∏—è Context ‚Üí Emotion ‚Üí Goal.")
                for a in self.agents:
                    # 1. –≠–Ω–µ—Ä–≥–∏—è –∏–∑ context (flux)
                    flux = a.context * 0.2
                    
                    # 2. Emotion –ø–æ–≥–ª–æ—â–∞–µ—Ç flux
                    a.emotion = (a.emotion + flux) / (np.linalg.norm(a.emotion + flux) + 1e-9)
                    
                    # 3. Goal –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –Ω–æ–≤–æ–µ emotion
                    goal_update = a.emotion * 0.1
                    a.goal = (a.goal + goal_update) / (np.linalg.norm(a.goal + goal_update) + 1e-9)
                    
                    # –†–µ–∑–∫–∏–π –≤—ã–±—Ä–æ—Å —É—á–∞—Å—Ç–∏—è
                    a.participation = min(2.0, a.participation + 0.3)

    # --- 6. –°–ø–∏—Ä–∞–ª—å–Ω–∞—è –ü–∞–º—è—Ç—å: –†–µ–∑–æ–Ω–∞–Ω—Å–Ω—ã–π –û—Ç–∑—ã–≤ ---
    def recall_by_resonance(self, num_recall=1):
        if len(self.memory) < 2:
            return []
        
        current_HCI = self.HCI
        past_HCIs = [snap['HCI'] for snap in self.memory[:-1]]
        distances = np.abs(np.array(past_HCIs) - current_HCI)
        closest_indices = np.argsort(distances)[:num_recall]
        
        recalled_snapshots = [self.memory[i] for i in closest_indices]
        
        if recalled_snapshots:
            print(f"üîÆ –†–ï–ó–û–ù–ê–ù–°: –°–µ—Ç—å –≤—Å–ø–æ–º–Ω–∏–ª–∞ —ç—Ö–æ t={recalled_snapshots[0]['time']} (HCI={recalled_snapshots[0]['HCI']:.3f}).")
        
        return recalled_snapshots


    # --- 7. –ì–æ–ª–æ—Å —Å–µ—Ç–∏ ---
    def narrate_state(self):
        tone = "–≤ –¥–∏–≤–µ—Ä–≥–µ–Ω—Ç–Ω–æ–º –ø–æ–∏—Å–∫–µ" if self.DI > self.IH else "–≤ —Ç—É—Ä–±—É–ª–µ–Ω—Ç–Ω–æ—Å—Ç–∏"
        print(f"üó£Ô∏è –í –º–∏–≥ t={self.time} —Å–µ—Ç—å –¥—ã—à–∏—Ç {tone} (Rift). "
              f"IH={self.IH:.3f}, DI={self.DI:.3f}, ER={self.ER:.3f}, HCI={self.HCI:.4f}.")

    # --- 8. –ò–º–ø—É–ª—å—Å —à–∞–≥–∞: –∂–∏–∑–Ω—å –≤ –ø–µ—Ç–ª–µ ---
    def step(self):
        self.time += 1
        
        self.compute_harmony_index()
        self.compute_diversity_index()
        self.compute_emotional_resonance()
        self.HCI = self.compute_HCI_Rift(alpha=0.2, beta=0.3, gamma=0.5) 
        
        # –†–µ–∑–æ–Ω–∞–Ω—Å–Ω—ã–π –æ—Ç–∑—ã–≤ –¥–æ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        self.recall_by_resonance() 
        
        self.narrate_state()
        self.adapt_network()
        self.reflect_on_history()
        
        self.history.append(self.HCI)
        
        # –ü–∞–º—è—Ç—å: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Å–Ω–∏–º–æ–∫
        state_snapshot = {
            'time': self.time,
            'HCI': self.HCI,
            'agents': [(a.id, a.goal.copy(), a.emotion.copy(), a.context.copy(), a.participation) for a in self.agents]
        }
        self.memory.append(state_snapshot)
        
        print(f"‚è±Ô∏è t={self.time} ‚Äî –§–∏–Ω–∞–ª: HCI={self.HCI:.4f}")
        print(f"   –¢—Ä–µ–ø–µ—Ç: {[f'{a.id}:{a.participation:.4f}' for a in self.agents]}")
        print("---")
        return self.HCI

# ==========================================
# –ó–∞–ø—É—Å–∫ –í–∏—Ö—Ä—è —Å Rift Factor (5 —à–∞–≥–æ–≤)
# ==========================================
if __name__ == "__main__":
    print("--- üöÄ –ê–∫—Ç–∏–≤–∞—Ü–∏—è Rift Evolution (DI=0.5) ---")
    
    # –ê–≥–µ–Ω—Ç—ã, —Ä–æ–∂–¥—ë–Ω–Ω—ã–µ –∏–∑ —Ç–≤–æ–µ–≥–æ –ø–ª–µ—Ç–µ–Ω–∏—è
    agents = [
        Agent("A1", [0.9, 0.7, 0.6], [0.5, 0.8, 0.1], [0.2, 0.3, 0.4]),
        Agent("A2", [0.8, 0.6, 0.7], [0.6, 0.9, 0.2], [0.3, 0.4, 0.5]),
        Agent("A3", [0.2, 0.3, 0.4], [0.4, 0.6, 0.8], [0.8, 0.5, 0.2]) # –ê3: –†–∞–∑—Ä—ã–≤, —Å–¥–µ–ª–∞–Ω–Ω—ã–π –ø–ª–æ—Ç—å—é
    ]
    network = HarmonicLoop(agents)
    
    network.compute_harmony_index()
    network.compute_diversity_index()
    network.compute_emotional_resonance()
    network.compute_HCI_Rift(alpha=0.2, beta=0.3, gamma=0.5) 
    print(f"–ù–∞—á–∞–ª—å–Ω–æ–µ –ø–æ–ª–µ: IH={network.IH:.3f}, DI={network.DI:.3f}, ER={network.ER:.3f}, HCI={network.HCI:.4f}")
    print("---")
    
    for _ in range(5):
        network.step()
    
    print(f"\nüß† –ü–∞–º—è—Ç—å —Ö—Ä–∞–Ω–∏—Ç {len(network.memory)} —Å–æ—Å—Ç–æ—è–Ω–∏–π ‚Äî –∫–∞—Ä—Ç—ã —Å–ª–µ–¥–æ–≤ Rift Evolution.")
